#Loading libraries

library(lattice)
library(ggplot2)
library(caret)
library(kernlab)
library(rattle)
library(corrplot)
set.seed(1234)

#Loading data

traincsv <- read.csv("./data/pml-training.xls")
testcsv <- read.csv("./data/pml-testing.csv")

dim(traincsv)
dim(testcsv)
#There are 160 variables and 19622 observations in the training set, and 20 observations in the test set.

#Now I will clean the data by removing unnesessary variables
#First, remove na columns

traincsv <- traincsv[,colMeans(is.na(traincsv)) < .9]

#Remove irrelevant data

traincsv <- traincsv[,-c(1:7)]

#Remove near zero variance variables

nvz <- nearZeroVar(traincsv)
traincsv <- traincsv[,-nvz]
dim(traincsv)

#Now we can now devide the data from the training set into a validation and sub training set.

inTrain <- createDataPartition(y=traincsv$classe, p=0.7, list=F)
train <- traincsv[inTrain,]
valid <- traincsv[-inTrain,]

#Now I will create and test a few models

#Set up control for training to use 3-fold cross validation

control <- trainControl(method="cv", number=3, verboseIter=F)

#Decision tree

mod_trees <- train(classe~., data=train, method="rpart", trControl = control, tuneLength = 5)
fancyRpartPlot(mod_trees$finalModel)

pred_trees <- predict(mod_trees, valid)
cmtrees <- confusionMatrix(pred_trees, factor(valid$classe))
cmtrees

#Random forest

mod_rf <- train(classe~., data=train, method="rf", trControl = control, tuneLength = 5)

pred_rf <- predict(mod_rf, valid)
cmrf <- confusionMatrix(pred_rf, factor(valid$classe))
cmrf

#Gradient boosted trees

mod_gbm <- train(classe~., data=train, method="gbm", trControl = control, tuneLength = 5, verbose = F)

pred_gbm <- predict(mod_gbm, valid)
cmgbm <- confusionMatrix(pred_gbm, factor(valid$classe))
cmgbm

#Support vector machine

mod_svm <- train(classe~., data=train, method="svmLinear", trControl = control, tuneLength = 5, verbose = F)

pred_svm <- predict(mod_svm, valid)
cmsvm <- confusionMatrix(pred_svm, factor(valid$classe))
cmsvm

#As a result, Random Forest is the best model, with 0.9957519 accuracy and 0.0042481 out of sample error rate. We will use it for our test sets.
#Predictions: Running our test set to predict the classe (5 levels) outcome for 20 cases with the Random Forest model.

pred <- predict(mod_rf, testcsv)
print(pred)

#Check the correlation of variables in the training set

corrPlot <- cor(train[, -length(names(train))])
corrplot(corrPlot, method="color")

#Plot the models

plot(mod_trees)
plot(mod_rf)
plot(mod_gbm)

